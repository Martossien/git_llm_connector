"""
title: Git LLM Connector
author: Martossien
author_url: https://github.com/Martossien
git_url: https://github.com/Martossien/git_llm_connector
description: Tool Open WebUI pour cloner, analyser et r√©sumer des d√©p√¥ts Git √† l'aide de LLM accessibles via les CLI Gemini ou Qwen.
required_open_webui_version: 0.6.0
version: 0.1.0
license: MIT
requirements: aiofiles pathspec pydantic
"""

from typing import Optional, Callable, Awaitable, Any, List, Dict, Union
from pydantic import BaseModel, Field
import asyncio
import aiofiles
import os
import json
import subprocess
import shutil
import glob
import pathspec
import logging
import re
from datetime import datetime
from urllib.parse import urlparse
from pathlib import Path

class Tools:
    """
    Git LLM Connector - Tool Open WebUI pour l'analyse intelligente de d√©p√¥ts Git
    
    Ce tool permet de :
    - Cloner et synchroniser des d√©p√¥ts GitHub/GitLab
    - Analyser automatiquement le code via des LLM CLI externes
    - G√©n√©rer des synth√®ses structur√©es (ARCHITECTURE.md, API_SUMMARY.md, etc.)
    - Injecter intelligemment le contexte dans les conversations
    
    Architecture :
    ~/OW_tools/
    ‚îú‚îÄ‚îÄ git_llm_connector.py (ce fichier)
    ‚îú‚îÄ‚îÄ git_repos/
    ‚îÇ   ‚îî‚îÄ‚îÄ {owner}_{repo}/
    ‚îÇ       ‚îú‚îÄ‚îÄ docs_analysis/
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ API_SUMMARY.md
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ CODE_MAP.md
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ analysis_metadata.json
    ‚îÇ       ‚îî‚îÄ‚îÄ [fichiers du repo]
    ‚îú‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ README.md
    """
    
    class Valves(BaseModel):
        """
        Configuration globale du Git LLM Connector (niveau administrateur)
        
        Ces param√®tres d√©finissent le comportement par d√©faut du tool
        et peuvent √™tre ajust√©s par l'administrateur Open WebUI.
        """
        
        git_repos_path: str = Field(
            default="~/OW_tools/git_repos",
            description="R√©pertoire racine de stockage des d√©p√¥ts Git clon√©s"
        )
        
        default_timeout: int = Field(
            default=300,
            description="Timeout par d√©faut pour les op√©rations Git et LLM CLI (secondes)"
        )
        
        default_globs_include: str = Field(
            default="**/*.py,**/*.js,**/*.ts,**/*.jsx,**/*.tsx,**/*.vue,**/*.go,**/*.rs,**/*.java,**/*.cpp,**/*.c,**/*.h,**/*.md,**/*.txt,**/*.yml,**/*.yaml,**/*.json,**/*.toml,**/*.cfg,**/*.ini",
            description="Patterns de fichiers √† inclure par d√©faut dans l'analyse"
        )
        
        default_globs_exclude: str = Field(
            default="**/.git/**,**/node_modules/**,**/dist/**,**/build/**,**/__pycache__/**,**/target/**,**/.venv/**,**/venv/**,**/*.png,**/*.jpg,**/*.jpeg,**/*.gif,**/*.svg,**/*.ico,**/*.pdf,**/*.zip,**/*.tar.gz",
            description="Patterns de fichiers √† exclure par d√©faut de l'analyse"
        )
        
        max_file_size_kb: int = Field(
            default=500,
            description="Taille maximale des fichiers √† analyser (Ko)"
        )
        
        enable_debug_logging: bool = Field(
            default=True,
            description="Activer les logs d√©taill√©s pour le d√©bogage"
        )
        
        supported_git_hosts: str = Field(
            default="github.com,gitlab.com",
            description="H√¥tes Git support√©s (s√©par√©s par des virgules)"
        )

        max_context_bytes: int = Field(
            default=32 * 1024 * 1024,
            description="Taille maximale du contexte envoy√© au LLM (octets)"
        )

        max_bytes_per_file: int = Field(
            default=512 * 1024,
            description="Taille maximale lue par fichier pour le contexte (octets)"
        )

        extra_bin_dirs: str = Field(
            default="",
            description="Chemins additionnels pour les binaires LLM (s√©par√©s par ':')"
        )

        git_timeout_s: float = Field(
            default=120.0,
            description="Timeout pour les op√©rations Git (secondes)"
        )

        llm_timeout_s: float = Field(
            default=180.0,
            description="Timeout pour les appels LLM CLI (secondes)"
        )

    class UserValves(BaseModel):
        """
        Configuration utilisateur du Git LLM Connector
        
        Ces param√®tres permettent √† chaque utilisateur de personnaliser
        le comportement du tool selon ses pr√©f√©rences.
        """
        
        llm_cli_choice: str = Field(
            default="qwen",
            description="Choix du LLM CLI pour l'analyse (qwen, gemini, ou auto)"
        )
        
        enable_auto_analysis: bool = Field(
            default=True,
            description="Activer l'analyse automatique par LLM CLI lors du clonage"
        )
        
        max_context_files: int = Field(
            default=10,
            description="Nombre maximum de fichiers de synth√®se √† injecter dans le contexte"
        )
        
        custom_globs_include: str = Field(
            default="",
            description="Patterns personnalis√©s de fichiers √† inclure (laissez vide pour utiliser les d√©fauts)"
        )
        
        custom_globs_exclude: str = Field(
            default="",
            description="Patterns personnalis√©s de fichiers √† exclure (laissez vide pour utiliser les d√©fauts)"
        )
        
        analysis_depth: str = Field(
            default="standard",
            description="Profondeur d'analyse (quick, standard, deep)"
        )
        
        preferred_language: str = Field(
            default="fr",
            description="Langue pr√©f√©r√©e pour les synth√®ses g√©n√©r√©es (fr, en)"
        )

        llm_bin_name: str = Field(
            default="gemini",
            description="Nom du binaire LLM √† utiliser (gemini ou qwen)"
        )

        llm_model_name: str = Field(
            default="gemini-2.5-pro",
            description="Nom du mod√®le LLM √† utiliser"
        )

        llm_cmd_template: str = Field(
            default="{bin} --model {model} --prompt {prompt}",
            description="Gabarit de commande pour l'appel LLM CLI"
        )

    def __init__(self):
        """
        Initialise le Git LLM Connector
        
        Configure les valves, initialise le syst√®me de logging,
        et pr√©pare l'environnement de travail.
        """
        self.valves = self.Valves()
        self.user_valves = self.UserValves()
        self.citation = False  # Open WebUI g√®re automatiquement les citations
        
        # Initialisation du syst√®me de logging
        self._setup_logging()
        
        # Cache des m√©tadonn√©es des repos analys√©s
        self._repo_cache = {}
        
        # Patterns de prompts pour les diff√©rents LLM CLI
        self._analysis_prompts = {
            "architecture": {
                "fr": "Analyse l'architecture de ce projet de code. D√©cris la stack technique utilis√©e, les modules principaux, les points d'entr√©e de l'application, l'organisation g√©n√©rale du code, et les patterns architecturaux identifi√©s. Sois concis mais complet.",
                "en": "Analyze the architecture of this code project. Describe the technical stack used, main modules, application entry points, general code organization, and identified architectural patterns. Be concise but comprehensive."
            },
            "api": {
                "fr": "Extrait et documente toutes les APIs, fonctions publiques, classes principales et leurs interfaces dans ce projet. Cr√©e un r√©sum√© des points d'entr√©e programmatiques disponibles.",
                "en": "Extract and document all APIs, public functions, main classes and their interfaces in this project. Create a summary of available programmatic entry points."
            },
            "codemap": {
                "fr": "Cr√©e une carte synth√©tique du code de ce projet : d√©cris le r√¥le de chaque dossier principal, identifie les fichiers les plus importants, et explique les flux de donn√©es principaux. Aide-moi √† naviguer efficacement dans ce codebase.",
                "en": "Create a synthetic code map of this project: describe the role of each main folder, identify the most important files, and explain the main data flows. Help me navigate efficiently through this codebase."
            }
        }
        
        self.logger.info("Git LLM Connector initialis√© avec succ√®s")

    def _setup_logging(self) -> None:
        """
        Configure le syst√®me de logging avanc√©
        
        Cr√©e un logger personnalis√© avec rotation des fichiers,
        niveaux de verbosit√© configurables, et formatage structur√©.
        """
        # Expansion du r√©pertoire home et cr√©ation du dossier logs
        log_dir = os.path.expanduser("~/OW_tools/logs")
        os.makedirs(log_dir, exist_ok=True)
        
        # Nom du fichier de log avec timestamp
        log_file = os.path.join(
            log_dir, 
            f"git_llm_connector_{datetime.now().strftime('%Y%m%d')}.log"
        )
        
        # Configuration du logger principal
        self.logger = logging.getLogger("GitLLMConnector")
        self.logger.setLevel(logging.DEBUG if self.valves.enable_debug_logging else logging.INFO)
        
        # √âviter les handlers dupliqu√©s
        if not self.logger.handlers:
            # Handler pour fichier avec rotation
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)
            
            # Handler pour console (optionnel selon environnement)
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.WARNING)
            
            # Formatage structur√© des logs
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)
        
        self.logger.info(f"Syst√®me de logging configur√© - Fichier: {log_file}")

    async def analyze_repo(
        self, 
        repo_url: str,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        Fonction principale d'analyse d'un d√©p√¥t Git
        
        Cette fonction orchestre l'ensemble du processus :
        1. Parse et validation de l'URL
        2. Clone ou mise √† jour du d√©p√¥t
        3. Scan des fichiers selon les patterns
        4. Analyse via LLM CLI externe
        5. G√©n√©ration des fichiers de synth√®se
        6. Injection dans le contexte
        
        Args:
            repo_url (str): URL compl√®te du d√©p√¥t Git (GitHub/GitLab)
            __event_emitter__: Fonction d'√©mission d'√©v√©nements Open WebUI
            
        Returns:
            str: R√©sum√© de l'analyse avec statistiques et contexte inject√©
            
        Raises:
            ValueError: Si l'URL est invalide ou non support√©e
            RuntimeError: Si l'analyse √©choue
        """
        try:
            self.logger.info(f"üöÄ D√©marrage analyse repo: {repo_url}")
            
            # √âmission du statut de d√©marrage
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "üîç Analyse du d√©p√¥t Git en cours...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            # 1. Parse et validation de l'URL
            repo_info = await self._parse_git_url(repo_url)
            self.logger.debug(f"Info repo pars√©e: {repo_info}")
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": f"üì• Clonage de {repo_info['owner']}/{repo_info['repo']}...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            # 2. Clone ou mise √† jour du d√©p√¥t
            local_path = await self._clone_or_update_repo(repo_info, __event_emitter__)
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "üìä Scan des fichiers du projet...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            # 3. Scan des fichiers selon les patterns
            file_stats = await self._scan_repository_files(local_path)
            self.logger.info(f"Fichiers scann√©s: {file_stats['total_files']} fichiers, {file_stats['total_size_mb']:.1f} MB")
            
            # 4. Analyse LLM CLI si activ√©e
            synthesis_files = []
            if self.user_valves.enable_auto_analysis:
                if __event_emitter__:
                    await __event_emitter__({
                        "type": "status",
                        "data": {
                            "description": f"ü§ñ Analyse par {self.user_valves.llm_cli_choice.upper()}...",
                            "done": False,
                            "hidden": False
                        }
                    })
                
                synthesis_files = await self._run_llm_analysis(local_path, repo_info, __event_emitter__)
            
            # 5. Injection du contexte
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "üìã Injection du contexte...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            context_content = await self._inject_repository_context(local_path, repo_info, synthesis_files, __event_emitter__)
            
            # 6. Finalisation
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "‚úÖ Analyse termin√©e avec succ√®s !",
                        "done": True,
                        "hidden": False
                    }
                })
            
            # Pr√©paration du r√©sum√© final
            summary = f"""
## üìä Analyse du d√©p√¥t {repo_info['owner']}/{repo_info['repo']} termin√©e

**Statistiques :**
- üìÅ Fichiers analys√©s : {file_stats['total_files']}
- üì¶ Taille totale : {file_stats['total_size_mb']:.1f} MB
- üóÇÔ∏è Types de fichiers : {', '.join(file_stats['file_types'][:5])}
- ü§ñ LLM utilis√© : {self.user_valves.llm_cli_choice.upper()}
- üìã Synth√®ses g√©n√©r√©es : {len(synthesis_files)}

Le contexte du d√©p√¥t a √©t√© inject√© et est maintenant disponible pour vos questions !
"""
            
            self.logger.info(f"‚úÖ Analyse termin√©e avec succ√®s pour {repo_url}")
            return summary
            
        except Exception as e:
            error_msg = f"‚ùå Erreur lors de l'analyse du d√©p√¥t: {str(e)}"
            self.logger.error(f"Erreur analyse repo {repo_url}: {e}", exc_info=True)
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": error_msg,
                        "done": True,
                        "hidden": False
                    }
                })
            
            return error_msg

    async def sync_repo(
        self,
        repo_name: str,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        Synchronise un d√©p√¥t d√©j√† clon√©
        
        Met √† jour un d√©p√¥t existant et relance l'analyse si des changements
        sont d√©tect√©s. Optimis√© pour les mises √† jour incr√©mentales.
        
        Args:
            repo_name (str): Nom du repo au format "owner_repo"
            __event_emitter__: Fonction d'√©mission d'√©v√©nements
            
        Returns:
            str: R√©sultat de la synchronisation
        """
        try:
            self.logger.info(f"üîÑ Synchronisation repo: {repo_name}")
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": f"üîÑ Synchronisation de {repo_name}...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            # V√©rification existence du repo local
            repos_path = os.path.expanduser(self.valves.git_repos_path)
            repo_path = os.path.join(repos_path, repo_name)
            
            if not os.path.exists(repo_path):
                raise ValueError(f"D√©p√¥t {repo_name} non trouv√© localement")
            
            # Git pull
            result = await self._run_git_command(repo_path, ["pull", "origin"], __event_emitter__)
            
            # V√©rification des changements
            has_changes = "Already up to date" not in result
            
            if has_changes and self.user_valves.enable_auto_analysis:
                # Relancer l'analyse
                synthesis_files = await self._run_llm_analysis(
                    repo_path, 
                    {"owner": repo_name.split("_")[0], "repo": "_".join(repo_name.split("_")[1:])},
                    __event_emitter__
                )
                
                if __event_emitter__:
                    await __event_emitter__({
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Synchronisation et analyse termin√©es !",
                            "done": True,
                            "hidden": False
                        }
                    })
                
                return f"‚úÖ D√©p√¥t {repo_name} synchronis√© et r√©-analys√© ({len(synthesis_files)} synth√®ses mises √† jour)"
            else:
                if __event_emitter__:
                    await __event_emitter__({
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Synchronisation termin√©e (aucun changement)",
                            "done": True,
                            "hidden": False
                        }
                    })
                
                return f"‚úÖ D√©p√¥t {repo_name} d√©j√† √† jour"
                
        except Exception as e:
            error_msg = f"‚ùå Erreur synchronisation {repo_name}: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": error_msg,
                        "done": True,
                        "hidden": False
                    }
                })
            
            return error_msg

    async def list_analyzed_repos(
        self,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        Liste tous les d√©p√¥ts analys√©s avec leurs m√©tadonn√©es
        
        Parcourt le r√©pertoire des d√©p√¥ts et collecte les informations
        sur chaque analyse effectu√©e.
        
        Returns:
            str: Liste format√©e des d√©p√¥ts avec statistiques
        """
        try:
            self.logger.info("üìã Listing des d√©p√¥ts analys√©s")
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "üìã Collecte des informations des d√©p√¥ts...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            repos_path = os.path.expanduser(self.valves.git_repos_path)
            
            if not os.path.exists(repos_path):
                return "üìÅ Aucun d√©p√¥t analys√© pour le moment"
            
            repos = []
            for item in os.listdir(repos_path):
                repo_path = os.path.join(repos_path, item)
                if os.path.isdir(repo_path):
                    metadata = await self._get_repo_metadata(repo_path)
                    repos.append({
                        "name": item,
                        "path": repo_path,
                        "metadata": metadata
                    })
            
            if not repos:
                return "üìÅ Aucun d√©p√¥t analys√© pour le moment"
            
            # Formatage de la liste
            result = f"## üìã D√©p√¥ts Git analys√©s ({len(repos)} total)\n\n"
            
            for repo in sorted(repos, key=lambda x: x['metadata'].get('last_analysis', '')):
                meta = repo['metadata']
                result += f"### üì¶ {repo['name']}\n"
                result += f"- üïí Derni√®re analyse : {meta.get('last_analysis', 'Non disponible')}\n"
                result += f"- üìÅ Fichiers : {meta.get('file_count', 'N/A')}\n"
                result += f"- üíæ Taille : {meta.get('total_size_mb', 'N/A')} MB\n"
                result += f"- ü§ñ LLM : {meta.get('llm_used', 'N/A')}\n"
                result += f"- üìã Synth√®ses : {meta.get('synthesis_count', 0)}\n\n"
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "‚úÖ Liste g√©n√©r√©e avec succ√®s",
                        "done": True,
                        "hidden": False
                    }
                })
            
            return result
            
        except Exception as e:
            error_msg = f"‚ùå Erreur listing repos: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return error_msg

    async def get_repo_context(
        self,
        repo_name: str,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        R√©cup√®re et injecte le contexte d'un d√©p√¥t sp√©cifique
        
        Charge les fichiers de synth√®se d'un d√©p√¥t d√©j√† analys√©
        et les injecte dans le contexte de la conversation.
        
        Args:
            repo_name (str): Nom du repo au format "owner_repo"
            
        Returns:
            str: Contexte inject√© ou message d'erreur
        """
        try:
            self.logger.info(f"üìã R√©cup√©ration contexte pour: {repo_name}")
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": f"üìã Chargement du contexte de {repo_name}...",
                        "done": False,
                        "hidden": False
                    }
                })
            
            repos_path = os.path.expanduser(self.valves.git_repos_path)
            repo_path = os.path.join(repos_path, repo_name)
            analysis_path = os.path.join(repo_path, "docs_analysis")
            
            if not os.path.exists(analysis_path):
                return f"‚ùå Aucune analyse trouv√©e pour {repo_name}. Lancez d'abord analyze_repo()."
            
            # Chargement des fichiers de synth√®se
            synthesis_files = []
            for filename in ["ARCHITECTURE.md", "API_SUMMARY.md", "CODE_MAP.md"]:
                file_path = os.path.join(analysis_path, filename)
                if os.path.exists(file_path):
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        synthesis_files.append({
                            "name": filename,
                            "content": content
                        })
            
            # Injection du contexte via citations
            if synthesis_files and __event_emitter__:
                for syn_file in synthesis_files:
                    await __event_emitter__({
                        "type": "citation",
                        "data": {
                            "document": [syn_file["content"]],
                            "metadata": [{"source": f"{repo_name}/{syn_file['name']}", "html": False}],
                            "source": {"name": f"{repo_name} - {syn_file['name']}"}
                        }
                    })
            
            if __event_emitter__:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": "‚úÖ Contexte inject√© avec succ√®s",
                        "done": True,
                        "hidden": False
                    }
                })
            
            return f"‚úÖ Contexte de {repo_name} charg√© ({len(synthesis_files)} fichiers de synth√®se inject√©s)"
            
        except Exception as e:
            error_msg = f"‚ùå Erreur chargement contexte {repo_name}: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return error_msg

    async def _parse_git_url(self, url: str) -> Dict[str, str]:
        """
        Parse et valide une URL Git
        
        Extrait les informations owner/repo d'une URL GitHub/GitLab
        et valide que l'h√¥te est support√©.
        
        Args:
            url (str): URL du d√©p√¥t Git
            
        Returns:
            Dict contenant owner, repo, host, et url_clean
            
        Raises:
            ValueError: Si l'URL est invalide ou non support√©e
        """
        try:
            # Nettoyage de l'URL
            url_clean = url.strip().rstrip('/')
            
            # Support des URLs avec ou sans protocole
            if not url_clean.startswith(('http://', 'https://', 'git@')):
                url_clean = f"https://{url_clean}"
            
            # Parse de l'URL
            if url_clean.startswith('git@'):
                # Format SSH: git@host:owner/repo.git
                match = re.match(r'git@([^:]+):([^/]+)/([^/]+?)(?:\.git)?$', url_clean)
                if not match:
                    raise ValueError("Format SSH invalide")
                host, owner, repo = match.groups()
                url_clean = f"https://{host}/{owner}/{repo}"
            else:
                # Format HTTPS
                parsed = urlparse(url_clean)
                host = parsed.netloc
                path_parts = [p for p in parsed.path.split('/') if p]
                
                if len(path_parts) < 2:
                    raise ValueError("URL doit contenir owner/repo")
                
                owner, repo = path_parts[0], path_parts[1]
                
                # Suppression du .git si pr√©sent
                if repo.endswith('.git'):
                    repo = repo[:-4]
            
            # Validation de l'h√¥te
            supported_hosts = [h.strip() for h in self.valves.supported_git_hosts.split(',')]
            if host not in supported_hosts:
                raise ValueError(f"H√¥te {host} non support√©. H√¥tes support√©s: {', '.join(supported_hosts)}")
            
            result = {
                "owner": owner,
                "repo": repo,
                "host": host,
                "url_clean": url_clean,
                "repo_name": f"{owner}_{repo}"
            }
            
            self.logger.debug(f"URL pars√©e: {result}")
            return result
            
        except Exception as e:
            raise ValueError(f"URL Git invalide '{url}': {str(e)}")

    async def _clone_or_update_repo(
        self, 
        repo_info: Dict[str, str],
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        Clone un nouveau d√©p√¥t ou met √† jour un existant
        
        Args:
            repo_info (Dict): Informations du d√©p√¥t (r√©sultat de _parse_git_url)
            __event_emitter__: Fonction d'√©mission d'√©v√©nements
            
        Returns:
            str: Chemin local du d√©p√¥t
            
        Raises:
            RuntimeError: Si l'op√©ration Git √©choue
        """
        try:
            repos_path = os.path.expanduser(self.valves.git_repos_path)
            os.makedirs(repos_path, exist_ok=True)
            
            local_path = os.path.join(repos_path, repo_info["repo_name"])
            
            if os.path.exists(local_path):
                self.logger.info(f"Mise √† jour repo existant: {local_path}")
                # Mise √† jour
                await self._run_git_command(local_path, ["fetch", "origin"], __event_emitter__)
                await self._run_git_command(local_path, ["reset", "--hard", "origin/HEAD"], __event_emitter__)
            else:
                self.logger.info(f"Clonage nouveau repo: {repo_info['url_clean']} -> {local_path}")
                # Clone
                await self._run_git_command(
                    repos_path, 
                    ["clone", repo_info["url_clean"], repo_info["repo_name"]], 
                    __event_emitter__
                )
            
            return local_path
            
        except Exception as e:
            raise RuntimeError(f"√âchec clone/update repo: {str(e)}")

    async def _run_git_command(
        self, 
        cwd: str, 
        cmd: List[str],
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        Ex√©cute une commande Git de mani√®re asynchrone
        
        Args:
            cwd (str): R√©pertoire de travail
            cmd (List[str]): Commande Git √† ex√©cuter
            __event_emitter__: Fonction d'√©mission d'√©v√©nements
            
        Returns:
            str: Sortie de la commande
            
        Raises:
            RuntimeError: Si la commande √©choue
        """
        try:
            full_cmd = ["git"] + cmd
            self.logger.debug(f"Ex√©cution commande Git: {' '.join(full_cmd)} dans {cwd}")
            
            process = await asyncio.create_subprocess_exec(
                *full_cmd,
                cwd=cwd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(
                process.communicate(), 
                timeout=self.valves.default_timeout
            )
            
            if process.returncode != 0:
                error_output = stderr.decode('utf-8')
                raise RuntimeError(f"Commande Git √©chou√©e: {error_output}")
            
            result = stdout.decode('utf-8')
            self.logger.debug(f"R√©sultat Git: {result[:200]}...")
            return result
            
        except asyncio.TimeoutError:
            raise RuntimeError(f"Timeout commande Git apr√®s {self.valves.default_timeout}s")
        except Exception as e:
            raise RuntimeError(f"Erreur ex√©cution Git: {str(e)}")

    async def _scan_repository_files(self, repo_path: str) -> Dict[str, Any]:
        """
        Scan les fichiers du d√©p√¥t selon les patterns configur√©s
        
        Args:
            repo_path (str): Chemin du d√©p√¥t local
            
        Returns:
            Dict avec statistiques des fichiers scann√©s
        """
        try:
            self.logger.info(f"Scan fichiers repo: {repo_path}")
            
            # D√©termination des patterns √† utiliser
            include_patterns = (
                self.user_valves.custom_globs_include 
                if self.user_valves.custom_globs_include 
                else self.valves.default_globs_include
            ).split(',')
            
            exclude_patterns = (
                self.user_valves.custom_globs_exclude 
                if self.user_valves.custom_globs_exclude 
                else self.valves.default_globs_exclude
            ).split(',')
            
            # Cr√©ation des specs pathspec
            include_spec = pathspec.PathSpec.from_lines('gitwildmatch', include_patterns)
            exclude_spec = pathspec.PathSpec.from_lines('gitwildmatch', exclude_patterns)
            
            files_found = []
            total_size = 0
            file_types = set()
            
            # Parcours r√©cursif
            for root, dirs, files in os.walk(repo_path):
                # Filtrage des dossiers exclus
                dirs[:] = [d for d in dirs if not exclude_spec.match_file(os.path.join(root, d))]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, repo_path)
                    
                    # Application des patterns
                    if include_spec.match_file(rel_path) and not exclude_spec.match_file(rel_path):
                        try:
                            file_stat = os.stat(file_path)
                            file_size = file_stat.st_size
                            
                            # V√©rification taille maximale
                            if file_size <= self.valves.max_file_size_kb * 1024:
                                files_found.append({
                                    "path": rel_path,
                                    "size": file_size,
                                    "ext": os.path.splitext(file)[1].lower()
                                })
                                total_size += file_size
                                
                                # Collecte des types de fichiers
                                ext = os.path.splitext(file)[1].lower()
                                if ext:
                                    file_types.add(ext[1:])  # Sans le point
                                    
                        except OSError:
                            continue  # Fichier inaccessible
            
            stats = {
                "total_files": len(files_found),
                "total_size_bytes": total_size,
                "total_size_mb": total_size / (1024 * 1024),
                "file_types": sorted(list(file_types)),
                "files": files_found
            }
            
            self.logger.info(f"Scan termin√©: {stats['total_files']} fichiers, {stats['total_size_mb']:.1f} MB")
            return stats
            
        except Exception as e:
            self.logger.error(f"Erreur scan fichiers: {e}", exc_info=True)
            raise RuntimeError(f"√âchec scan fichiers: {str(e)}")

    async def _run_llm_analysis(
        self, 
        repo_path: str, 
        repo_info: Dict[str, str],
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> List[str]:
        """
        Lance l'analyse via LLM CLI externe
        
        Pr√©pare les prompts, ex√©cute le LLM CLI, et sauvegarde les r√©sultats
        dans des fichiers de synth√®se structur√©s.
        
        Args:
            repo_path (str): Chemin du d√©p√¥t local
            repo_info (Dict): Informations du d√©p√¥t
            __event_emitter__: Fonction d'√©mission d'√©v√©nements
            
        Returns:
            List des fichiers de synth√®se g√©n√©r√©s
        """
        try:
            self.logger.info(f"Analyse LLM du repo: {repo_path}")
            
            # Cr√©ation du dossier d'analyse
            analysis_dir = os.path.join(repo_path, "docs_analysis")
            os.makedirs(analysis_dir, exist_ok=True)
            
            # V√©rification de la disponibilit√© du LLM CLI
            llm_cli = await self._get_available_llm_cli()
            
            if not llm_cli:
                self.logger.warning("Aucun LLM CLI disponible, analyse ignor√©e")
                return []
            
            # Pr√©paration du contexte de code
            code_context = await self._prepare_code_context(repo_path)
            
            # G√©n√©ration des synth√®ses
            synthesis_files = []
            lang = self.user_valves.preferred_language
            
            analyses_to_run = [
                ("ARCHITECTURE.md", "architecture"),
                ("API_SUMMARY.md", "api"),
                ("CODE_MAP.md", "codemap")
            ]
            
            for filename, analysis_type in analyses_to_run:
                if __event_emitter__:
                    await __event_emitter__({
                        "type": "status",
                        "data": {
                            "description": f"ü§ñ G√©n√©ration {filename}...",
                            "done": False,
                            "hidden": False
                        }
                    })
                
                try:
                    prompt = self._analysis_prompts[analysis_type][lang]
                    result = await self._execute_llm_cli(llm_cli, prompt, code_context)
                    
                    if result:
                        file_path = os.path.join(analysis_dir, filename)
                        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                            await f.write(result)
                        
                        synthesis_files.append(filename)
                        self.logger.info(f"Synth√®se g√©n√©r√©e: {filename}")
                    
                except Exception as e:
                    self.logger.error(f"Erreur g√©n√©ration {filename}: {e}")
                    continue
            
            # Sauvegarde des m√©tadonn√©es
            await self._save_analysis_metadata(analysis_dir, repo_info, llm_cli, len(synthesis_files))
            
            self.logger.info(f"Analyse LLM termin√©e: {len(synthesis_files)} synth√®ses g√©n√©r√©es")
            return synthesis_files
            
        except Exception as e:
            self.logger.error(f"Erreur analyse LLM: {e}", exc_info=True)
            return []

    async def _get_available_llm_cli(self) -> Optional[str]:
        """
        D√©tecte le LLM CLI disponible
        
        Teste la disponibilit√© des diff√©rents LLM CLI selon la configuration
        utilisateur ou d√©tection automatique.
        
        Returns:
            Nom du LLM CLI disponible ou None
        """
        try:
            choice = self.user_valves.llm_cli_choice.lower()
            
            if choice == "auto":
                # Test automatique dans l'ordre de pr√©f√©rence
                for llm in ["qwen", "gemini"]:
                    if await self._test_llm_cli(llm):
                        self.logger.info(f"LLM CLI d√©tect√© automatiquement: {llm}")
                        return llm
                return None
            else:
                # Test du choix utilisateur
                if await self._test_llm_cli(choice):
                    return choice
                else:
                    self.logger.warning(f"LLM CLI {choice} non disponible")
                    return None
                    
        except Exception as e:
            self.logger.error(f"Erreur d√©tection LLM CLI: {e}")
            return None

    async def _test_llm_cli(self, llm_name: str) -> bool:
        """
        Test la disponibilit√© d'un LLM CLI sp√©cifique
        
        Args:
            llm_name (str): Nom du LLM CLI √† tester
            
        Returns:
            bool: True si disponible
        """
        try:
            # Commandes de test selon le LLM
            test_commands = {
                "qwen": ["qwen", "--version"],
                "gemini": ["gemini", "--version"]
            }
            
            if llm_name not in test_commands:
                return False
            
            cmd = test_commands[llm_name]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            await asyncio.wait_for(process.communicate(), timeout=10)
            return process.returncode == 0
            
        except Exception:
            return False

    def _redact_secrets(self, text: str) -> str:
        """Masque les secrets communs dans le texte fourni.

        Cette redaction simple remplace les valeurs sensibles par '****'.
        """
        try:
            # Paires clef=valeur classiques (.env)
            text = re.sub(
                r"(?i)(API_KEY|SECRET|TOKEN|PASSWORD|AWS_SECRET_ACCESS_KEY)\s*=\s*[^\s]+",
                lambda m: f"{m.group(1)}=****",
                text,
            )

            # Jetons GitHub personnels
            text = re.sub(r"ghp_[A-Za-z0-9]+", "ghp_****", text)

            # Tokens JWT basiques
            text = re.sub(
                r"eyJ[\w-]+?\.[\w-]+?\.[\w-]+",
                "****",
                text,
            )

            return text
        except Exception:
            # En cas de probl√®me de regex, retourner le texte original
            return text

    async def _prepare_code_context(self, repo_path: str) -> str:
        """
        Pr√©pare le contexte de code pour l'analyse LLM
        
        S√©lectionne et formate les fichiers les plus importants
        du d√©p√¥t pour alimenter l'analyse LLM.
        
        Args:
            repo_path (str): Chemin du d√©p√¥t
            
        Returns:
            str: Contexte format√© pour le LLM
        """
        try:
            max_ctx = self.valves.max_context_bytes
            max_file = self.valves.max_bytes_per_file

            context_parts: List[str] = []
            total_bytes = 0

            marker = "[... CONTEXTE TRONQU√â ...]"
            marker_bytes = len(marker.encode("utf-8"))

            def append_part(part: str) -> bool:
                nonlocal total_bytes
                part_bytes = len(part.encode("utf-8"))
                if total_bytes + part_bytes > max_ctx:
                    allowed = max_ctx - total_bytes - marker_bytes
                    if allowed > 0:
                        truncated = part.encode("utf-8")[:allowed].decode("utf-8", errors="ignore")
                        context_parts.append(truncated)
                        total_bytes += allowed
                    context_parts.append(marker)
                    total_bytes = max_ctx
                    return False
                context_parts.append(part)
                total_bytes += part_bytes
                return True

            # Fichiers prioritaires (README, package.json, etc.)
            priority_files = [
                "README.md", "README.rst", "README.txt",
                "package.json", "setup.py", "Cargo.toml", "go.mod",
                "requirements.txt", "pyproject.toml", "composer.json"
            ]

            for priority_file in priority_files:
                file_path = os.path.join(repo_path, priority_file)
                if os.path.exists(file_path):
                    try:
                        async with aiofiles.open(file_path, "rb") as f:
                            data = await f.read(max_file)
                        content = data.decode("utf-8", errors="ignore")
                        part = f"=== {priority_file} ===\n{content}\n"
                        if not append_part(part):
                            self.logger.info(f"Contexte tronqu√© √† {total_bytes} octets")
                            return self._redact_secrets("".join(context_parts))
                    except Exception:
                        continue

            # Scan des fichiers du projet selon la profondeur d'analyse
            file_stats = await self._scan_repository_files(repo_path)

            depth_limits = {"quick": 10, "standard": 25, "deep": 50}
            max_files = depth_limits.get(self.user_valves.analysis_depth, 25)
            selected_files = file_stats["files"][:max_files]

            for file_info in selected_files:
                file_path = os.path.join(repo_path, file_info["path"])
                try:
                    async with aiofiles.open(file_path, "rb") as f:
                        data = await f.read(max_file)
                    content = data.decode("utf-8", errors="ignore")
                    part = f"=== {file_info['path']} ===\n{content}\n"
                    if not append_part(part):
                        self.logger.info(f"Contexte tronqu√© √† {total_bytes} octets")
                        return self._redact_secrets("".join(context_parts))
                except Exception:
                    continue

            self.logger.info(f"Contexte total pr√©par√©: {total_bytes} octets")
            return self._redact_secrets("".join(context_parts))

        except Exception as e:
            self.logger.error(f"Erreur pr√©paration contexte: {e}")
            return ""

    async def _execute_llm_cli(self, llm_cli: str, prompt: str, context: str) -> Optional[str]:
        """
        Ex√©cute le LLM CLI avec le prompt et le contexte
        
        Args:
            llm_cli (str): Nom du LLM CLI √† utiliser
            prompt (str): Prompt d'analyse
            context (str): Contexte de code
            
        Returns:
            R√©sultat de l'analyse ou None si √©chec
        """
        try:
            # Pr√©paration de l'input complet
            full_input = f"{prompt}\n\nCONTEXTE DU CODE:\n{context}"
            
            # Commandes selon le LLM CLI
            if llm_cli == "qwen":
                cmd = ["qwen", "--input", "-"]
            elif llm_cli == "gemini":
                cmd = ["gemini", "generate", "--input", "-"]
            else:
                raise ValueError(f"LLM CLI non support√©: {llm_cli}")
            
            # Ex√©cution
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(
                process.communicate(input=full_input.encode('utf-8')),
                timeout=self.valves.default_timeout
            )
            
            if process.returncode == 0:
                result = stdout.decode('utf-8').strip()
                self.logger.debug(f"LLM CLI r√©sultat: {len(result)} caract√®res")
                return result
            else:
                error = stderr.decode('utf-8')
                self.logger.error(f"Erreur LLM CLI: {error}")
                return None
                
        except asyncio.TimeoutError:
            self.logger.error(f"Timeout LLM CLI apr√®s {self.valves.default_timeout}s")
            return None
        except Exception as e:
            self.logger.error(f"Erreur ex√©cution LLM CLI: {e}")
            return None

    async def _save_analysis_metadata(
        self, 
        analysis_dir: str, 
        repo_info: Dict[str, str], 
        llm_cli: str, 
        synthesis_count: int
    ) -> None:
        """
        Sauvegarde les m√©tadonn√©es de l'analyse
        
        Args:
            analysis_dir (str): R√©pertoire d'analyse
            repo_info (Dict): Informations du d√©p√¥t
            llm_cli (str): LLM CLI utilis√©
            synthesis_count (int): Nombre de synth√®ses g√©n√©r√©es
        """
        try:
            metadata = {
                "repo_info": repo_info,
                "analysis_timestamp": datetime.now().isoformat(),
                "llm_cli_used": llm_cli,
                "synthesis_count": synthesis_count,
                "user_config": {
                    "analysis_depth": self.user_valves.analysis_depth,
                    "preferred_language": self.user_valves.preferred_language
                },
                "tool_version": "1.0.0"
            }
            
            metadata_path = os.path.join(analysis_dir, "analysis_metadata.json")
            async with aiofiles.open(metadata_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(metadata, indent=2, ensure_ascii=False))
            
            self.logger.info(f"M√©tadonn√©es sauvegard√©es: {metadata_path}")
            
        except Exception as e:
            self.logger.error(f"Erreur sauvegarde m√©tadonn√©es: {e}")

    async def _inject_repository_context(
        self, 
        repo_path: str, 
        repo_info: Dict[str, str], 
        synthesis_files: List[str],
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None
    ) -> str:
        """
        Injecte le contexte du d√©p√¥t via les citations Open WebUI
        
        Charge les fichiers de synth√®se g√©n√©r√©s et les injecte
        dans la conversation via le syst√®me de citations.
        
        Args:
            repo_path (str): Chemin du d√©p√¥t
            repo_info (Dict): Informations du d√©p√¥t
            synthesis_files (List): Liste des fichiers de synth√®se
            __event_emitter__: Fonction d'√©mission d'√©v√©nements
            
        Returns:
            str: R√©sum√© du contexte inject√©
        """
        try:
            if not __event_emitter__:
                return "Event emitter non disponible pour injection"
            
            analysis_dir = os.path.join(repo_path, "docs_analysis")
            injected_files = 0
            
            # Injection des fichiers de synth√®se
            for filename in synthesis_files[:self.user_valves.max_context_files]:
                file_path = os.path.join(analysis_dir, filename)
                
                if os.path.exists(file_path):
                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                        
                        # Injection via citation
                        await __event_emitter__({
                            "type": "citation",
                            "data": {
                                "document": [content],
                                "metadata": [{
                                    "source": f"{repo_info['url_clean']}/{filename}",
                                    "html": False
                                }],
                                "source": {
                                    "name": f"{repo_info['owner']}/{repo_info['repo']} - {filename}"
                                }
                            }
                        })
                        
                        injected_files += 1
                        self.logger.debug(f"Contexte inject√©: {filename}")
                        
                    except Exception as e:
                        self.logger.error(f"Erreur injection {filename}: {e}")
            
            return f"Contexte inject√©: {injected_files} fichiers de synth√®se"
            
        except Exception as e:
            self.logger.error(f"Erreur injection contexte: {e}")
            return f"Erreur injection contexte: {str(e)}"

    async def _get_repo_metadata(self, repo_path: str) -> Dict[str, Any]:
        """
        R√©cup√®re les m√©tadonn√©es d'un d√©p√¥t analys√©
        
        Args:
            repo_path (str): Chemin du d√©p√¥t
            
        Returns:
            Dict avec les m√©tadonn√©es ou valeurs par d√©faut
        """
        try:
            metadata_path = os.path.join(repo_path, "docs_analysis", "analysis_metadata.json")
            
            if os.path.exists(metadata_path):
                async with aiofiles.open(metadata_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    metadata = json.loads(content)
                    
                    return {
                        "last_analysis": metadata.get("analysis_timestamp", "Inconnue"),
                        "llm_used": metadata.get("llm_cli_used", "Inconnue"),
                        "synthesis_count": metadata.get("synthesis_count", 0),
                        "file_count": "N/A",  # TODO: calculer depuis les stats
                        "total_size_mb": "N/A"
                    }
            else:
                return {
                    "last_analysis": "M√©tadonn√©es non disponibles",
                    "llm_used": "Inconnue",
                    "synthesis_count": 0,
                    "file_count": "N/A",
                    "total_size_mb": "N/A"
                }
                
        except Exception as e:
            self.logger.error(f"Erreur lecture m√©tadonn√©es {repo_path}: {e}")
            return {
                "last_analysis": f"Erreur: {str(e)}",
                "llm_used": "Erreur",
                "synthesis_count": 0,
                "file_count": "N/A",
                "total_size_mb": "N/A"
            }