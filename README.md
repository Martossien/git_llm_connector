# üöÄ Git LLM Connector - Tool Open WebUI

Un tool sophistiqu√© pour Open WebUI qui r√©volutionne l'interaction avec les d√©p√¥ts Git en combinant clonage intelligent, analyse par LLM CLI externe, et injection contextuelle automatique.

## üéØ Vue d'ensemble

Le Git LLM Connector va au-del√† des simples connecteurs GitHub existants en offrant :

- **üîÑ Clonage et synchronisation** automatique des d√©p√¥ts GitHub/GitLab
- **ü§ñ Analyse intelligente** par LLM CLI externes (Qwen, Gemini)
- **üìã G√©n√©ration de synth√®ses** structur√©es (ARCHITECTURE.md, API_SUMMARY.md, CODE_MAP.md)
- **üíæ Injection contextuelle** automatique dans les conversations
- **‚ö° Performance optimis√©e** avec cache local et op√©rations asynchrones

## üèóÔ∏è Architecture

```
~/OW_tools/
‚îú‚îÄ‚îÄ git_llm_connector.py      # Tool principal
‚îú‚îÄ‚îÄ git_repos/                # D√©p√¥ts clon√©s
‚îÇ   ‚îî‚îÄ‚îÄ {owner}_{repo}/
‚îÇ       ‚îú‚îÄ‚îÄ .git/             # D√©p√¥t Git
‚îÇ       ‚îú‚îÄ‚îÄ docs_analysis/    # Synth√®ses g√©n√©r√©es
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ API_SUMMARY.md
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ CODE_MAP.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ analysis_metadata.json
‚îÇ       ‚îî‚îÄ‚îÄ [fichiers du repo]
‚îú‚îÄ‚îÄ logs/                     # Logs d√©taill√©s
‚îÇ   ‚îî‚îÄ‚îÄ git_llm_connector_YYYYMMDD.log
‚îú‚îÄ‚îÄ README.md                 # Cette documentation
‚îî‚îÄ‚îÄ requirements.txt          # D√©pendances Python
```

## üîß Installation

### Pr√©requis syst√®me

1. **Git** install√© et configur√© :
```bash
git --version
git config --global user.name "Votre Nom"
git config --global user.email "votre.email@example.com"
```

2. **LLM CLI** au choix :

**Option A - Qwen CLI :**
```bash
pip install qwen-cli
# ou suivez les instructions sur https://github.com/QwenLM/Qwen
```

**Option B - Gemini CLI :**
```bash
pip install google-generativeai
# et configurez votre cl√© API
```

### Installation du Tool

1. **Clonez ou t√©l√©chargez** ce d√©p√¥t dans votre r√©pertoire Open WebUI tools

2. **Installez les d√©pendances Python :**
```bash
cd ~/OW_tools
pip install -r requirements.txt
```

3. **Configurez les permissions :**
```bash
chmod +x git_llm_connector.py
```

4. **Ajoutez le tool dans Open WebUI :**
   - Copiez `git_llm_connector.py` dans votre dossier Tools Open WebUI
   - Red√©marrez Open WebUI
   - Le tool appara√Ætra dans l'interface

## ‚öôÔ∏è Configuration

### Configuration Administrateur (Valves)

Accessible via l'interface admin d'Open WebUI :

| Param√®tre | D√©faut | Description |
|-----------|---------|-------------|
| `git_repos_path` | `~/OW_tools/git_repos` | R√©pertoire de stockage des repos |
| `default_timeout` | `300` | Timeout op√©rations (secondes) |
| `default_globs_include` | `**/*.py,**/*.js,**/*.ts,...` | Patterns fichiers inclus |
| `default_globs_exclude` | `**/.git/**,**/node_modules/**,...` | Patterns fichiers exclus |
| `max_file_size_kb` | `500` | Taille max fichiers analys√©s (Ko) |
| `enable_debug_logging` | `true` | Activation logs d√©taill√©s |
| `supported_git_hosts` | `github.com,gitlab.com` | H√¥tes Git support√©s |

### Configuration Utilisateur (UserValves)

Personnalisable par chaque utilisateur :

| Param√®tre | D√©faut | Description |
|-----------|---------|-------------|
| `llm_cli_choice` | `qwen` | LLM CLI √† utiliser (qwen/gemini/auto) |
| `enable_auto_analysis` | `true` | Analyse automatique lors du clone |
| `max_context_files` | `10` | Nb max fichiers inject√©s |
| `custom_globs_include` | `""` | Patterns personnalis√©s (inclusion) |
| `custom_globs_exclude` | `""` | Patterns personnalis√©s (exclusion) |
| `analysis_depth` | `standard` | Profondeur analyse (quick/standard/deep) |
| `preferred_language` | `fr` | Langue des synth√®ses (fr/en) |

## üöÄ Utilisation

### Fonctions principales

#### 1. `analyze_repo(repo_url)`

Analyse compl√®te d'un d√©p√¥t Git :

```python
# Exemples d'URLs support√©es
analyze_repo("https://github.com/owner/repo")
analyze_repo("https://gitlab.com/owner/project")
analyze_repo("github.com/owner/repo")  # HTTPS ajout√© automatiquement
analyze_repo("git@github.com:owner/repo.git")  # Format SSH
```

**Processus automatique :**
1. üîç Parse et validation de l'URL
2. üì• Clone ou mise √† jour du d√©p√¥t
3. üìä Scan des fichiers selon les patterns
4. ü§ñ Analyse via LLM CLI (si activ√©e)
5. üìã G√©n√©ration des synth√®ses
6. üíæ Injection dans le contexte

#### 2. `sync_repo(repo_name)`

Synchronisation d'un d√©p√¥t existant :

```python
sync_repo("facebook_react")  # Format: owner_repo
```

#### 3. `list_analyzed_repos()`

Liste tous les d√©p√¥ts analys√©s avec m√©tadonn√©es :

```python
list_analyzed_repos()
```

#### 4. `get_repo_context(repo_name)`

R√©injecte le contexte d'un d√©p√¥t :

```python
get_repo_context("microsoft_vscode")
```

### Exemples d'usage

**Analyse d'un nouveau projet :**
```
Utilisateur: Peux-tu analyser le d√©p√¥t https://github.com/vercel/next.js ?
Assistant: analyze_repo("https://github.com/vercel/next.js")
```

**Questions sur le code apr√®s analyse :**
```
Utilisateur: Explique-moi l'architecture de Next.js
Assistant: [Utilise automatiquement le contexte inject√© depuis ARCHITECTURE.md]
```

**Synchronisation p√©riodique :**
```
Utilisateur: Met √† jour le repo Next.js
Assistant: sync_repo("vercel_next.js")
```

## üìä Fichiers de synth√®se g√©n√©r√©s

### ARCHITECTURE.md
- Stack technique identifi√©e
- Modules et composants principaux
- Points d'entr√©e de l'application
- Organisation du code
- Patterns architecturaux

### API_SUMMARY.md  
- APIs publiques expos√©es
- Fonctions et m√©thodes principales
- Interfaces et classes importantes
- Points d'entr√©e programmatiques

### CODE_MAP.md
- R√¥le de chaque dossier principal
- Fichiers critiques √† conna√Ætre
- Flux de donn√©es identifi√©s
- Guide de navigation dans le code

### analysis_metadata.json
- Informations techniques de l'analyse
- Timestamp et version
- Configuration utilis√©e
- Statistiques de traitement

## üîç Syst√®me de logging

### Localisation des logs
```bash
# Logs quotidiens avec rotation
~/OW_tools/logs/git_llm_connector_YYYYMMDD.log
```

### Niveaux de logging
- **DEBUG** : D√©tails techniques complets
- **INFO** : √âtapes principales du processus  
- **WARNING** : Avertissements non bloquants
- **ERROR** : Erreurs avec stack trace

### Exemple de log
```
2024-09-12 14:30:15 - GitLLMConnector - INFO - [analyze_repo:120] - üöÄ D√©marrage analyse repo: https://github.com/facebook/react
2024-09-12 14:30:16 - GitLLMConnector - DEBUG - [_parse_git_url:250] - URL pars√©e: {'owner': 'facebook', 'repo': 'react', 'host': 'github.com'}
2024-09-12 14:30:45 - GitLLMConnector - INFO - [_run_llm_analysis:380] - Analyse LLM termin√©e: 3 synth√®ses g√©n√©r√©es
```

## üõ†Ô∏è D√©pannage

### Erreurs courantes

#### "Git non disponible"
```bash
# V√©rification
git --version

# Installation Ubuntu/Debian
sudo apt-get install git

# Installation CentOS/RHEL  
sudo yum install git
```

#### "LLM CLI non trouv√©"
```bash
# Test Qwen CLI
qwen --version

# Test Gemini CLI  
python -c "import google.generativeai; print('OK')"
```

#### "Erreur de permissions"
```bash
# Permissions r√©pertoire
chmod -R 755 ~/OW_tools/
mkdir -p ~/OW_tools/git_repos ~/OW_tools/logs
```

#### "Timeout op√©rations Git"
Augmentez `default_timeout` dans la configuration admin si vous travaillez avec de gros d√©p√¥ts.

### Logs de d√©bogage

Pour un d√©bogage approfondi, activez `enable_debug_logging` dans les Valves admin et consultez :

```bash
tail -f ~/OW_tools/logs/git_llm_connector_$(date +%Y%m%d).log
```

## üîí S√©curit√© et bonnes pratiques

### Authentification Git
- Les credentials Git sont g√©r√©s par votre configuration locale
- Aucun stockage de mots de passe dans le tool
- Support des cl√©s SSH et tokens personnels GitHub/GitLab

### Isolation des donn√©es
- Chaque d√©p√¥t est isol√© dans son propre dossier
- Les synth√®ses sont stock√©es localement uniquement
- Aucune transmission de code vers des APIs externes sans votre contr√¥le

### Limitations de ressources
- Taille maximale des fichiers configurable
- Patterns d'exclusion pour √©viter les fichiers binaires
- Timeouts configurables pour √©viter les blocages

## üìà Performance et optimisation

### Cache intelligent
- Les d√©p√¥ts clon√©s sont r√©utilis√©s
- D√©tection automatique des changements
- Analyse incr√©mentale quand possible

### Op√©rations asynchrones  
- Clone en arri√®re-plan avec progression
- Analyses LLM non-bloquantes
- Interface temps r√©el via event emitters

### Gestion m√©moire
- Traitement par chunks des gros fichiers
- Lib√©ration automatique des ressources
- Rotation des logs pour √©viter l'encombrement

## ü§ù Contribution et d√©veloppement

### Architecture du code
- **Classes principales** : `Tools`, `Valves`, `UserValves`
- **M√©thodes publiques** : 4 fonctions principales expos√©es
- **M√©thodes priv√©es** : 15+ fonctions utilitaires internes
- **Gestion d'erreurs** : Try/catch exhaustif avec logging

### Extension du tool
Pour ajouter support d'autres LLM CLI, modifiez :
1. `_test_llm_cli()` pour la d√©tection
2. `_execute_llm_cli()` pour l'ex√©cution  
3. `test_commands` pour les commandes de test

### Tests
```bash
# Tests manuels recommand√©s
python -c "
import asyncio
from git_llm_connector import Tools
tool = Tools()
print('Tool initialis√© avec succ√®s')
"
```

## üìÑ Licence et cr√©dits

- **Licence** : MIT
- **Auteur** : Claude Code Assistant  
- **Version** : 1.0.0
- **Compatibilit√©** : Open WebUI 0.6.0+

---

## üí° Questions fr√©quentes

**Q: Puis-je utiliser d'autres LLM CLI que Qwen/Gemini ?**
R: Actuellement seuls Qwen et Gemini sont support√©s. L'architecture permet d'ajouter facilement d'autres LLM CLI en modifiant les m√©thodes `_test_llm_cli` et `_execute_llm_cli`.

**Q: Les d√©p√¥ts priv√©s sont-ils support√©s ?**
R: Oui, si votre configuration Git locale peut y acc√©der (cl√©s SSH, tokens). Le tool utilise votre configuration Git existante.

**Q: Quelle est la taille maximale de d√©p√¥t support√©e ?**
R: Aucune limite stricte, mais les gros d√©p√¥ts (>1GB) peuvent n√©cessiter d'ajuster les timeouts et patterns d'exclusion.

**Q: Puis-je personnaliser les prompts d'analyse ?**
R: Actuellement les prompts sont int√©gr√©s dans le code. Une future version pourrait permettre la personnalisation via configuration.

**Q: Le tool fonctionne-t-il hors ligne ?**
R: Une fois les d√©p√¥ts clon√©s et les LLM CLI install√©s localement, oui. Seule la synchronisation initiale n√©cessite une connexion.

---

*Pour un support technique, consultez les logs d√©taill√©s dans `~/OW_tools/logs/` et v√©rifiez votre configuration des LLM CLI.*